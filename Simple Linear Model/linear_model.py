# -*- coding: utf-8 -*-
"""TensorFlow_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11EsCDRpNZNLh76haYtB7Rn3deot1n4sH

## Importing all Dependencies
"""

import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import OneHotEncoder
print(tf.__version__)

"""## Importing Mnist dataset"""

data=tf.keras.datasets.mnist
(trainX,trainY),(testX,testY)=data.load_data()
trainX=np.array([x.ravel() for x in trainX])
testX=np.array([x.ravel() for x in testX])
one_hot=OneHotEncoder(sparse=False)
trainY=one_hot.fit_transform(trainY.reshape(len(trainY),1))
testY=one_hot.fit_transform(testY.reshape(len(testY),1))
print("Shape of Data: ")
print("Training Examles: ",trainX.shape)
print("Testing Examples: ",testX.shape)
img_size=(28,28)
pixels=784
num_of_classes=10
batch_size=100

"""## Helper Function for Plotting Images and graph

---
"""

def plot_images(images, cls_true, cls_pred=None,w=3,h=3):
    assert len(images) == len(cls_true)==w*h
    fig, axes = plt.subplots(w, h)
    fig.subplots_adjust(hspace=0.3, wspace=0.3)
    for i, ax in enumerate(axes.flat):
        ax.imshow(images[i].reshape(img_size), cmap='binary')
        if cls_pred is None:
            xlabel = "True: {0}".format(cls_true[i])
        else:
            xlabel = "True: {0}, Pred: {1}".format(cls_true[i], cls_pred[i])
        ax.set_xlabel(xlabel)
        ax.set_xticks([])
        ax.set_yticks([])
    plt.show()

def plot_graph(data):
    plt.plot(data)
    plt.show()

"""## Plotting Some of Images with their True Labels"""

images=testX[0:9]
true_class=np.argmax(testY[0:9],axis=1)
plot_images(images,true_class)

"""## Initializing Graph Components (placeholders and variables)"""

x=tf.placeholder(tf.float32,[None,pixels])
y=tf.placeholder(tf.float32,[None,num_of_classes])
weight=tf.Variable(tf.random.normal([pixels,num_of_classes]))
bias=tf.Variable(tf.zeros([1,num_of_classes]))

"""## Define Model"""

y_pred=tf.matmul(x,weight)+bias
prediction=tf.nn.softmax(y_pred)

"""## Define Loss Function and accuracy"""

cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y,logits=y_pred)
loss = tf.reduce_sum(cross_entropy)
accurate = tf.equal(tf.argmax(prediction,axis=1), tf.argmax(y,axis=1))
accuracy = tf.reduce_mean(tf.cast(accurate, tf.float32))

"""## Defining Gradient Descent Optimizer"""

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss)

"""## Initialize Session and variables"""

session=tf.Session()
session.run(tf.global_variables_initializer())

feed_dict_test = {x: testX,y: testY}

"""## Helper Function to visualize and print accuracy"""

def print_accuracy():
    acc = session.run(accuracy, feed_dict=feed_dict_test)
    print("Accuracy on test-set: {0:.1%}".format(acc))

def print_confusion_matrix():
    prediction = session.run(y_pred, feed_dict=feed_dict_test)
    cls_true=np.argmax(prediction,axis=1)
    cm = confusion_matrix(y_true=np.argmax(testY,axis=1),y_pred=cls_true)
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.tight_layout()
    plt.colorbar()
    tick_marks = np.arange(num_of_classes)
    plt.xticks(tick_marks, range(num_of_classes))
    plt.yticks(tick_marks, range(num_of_classes))
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

"""## Train Function"""

def train(iterations):
    cost=[]
    for i in range(iterations):
        rnd_index=np.random.choice(len(trainX),batch_size)
        x_batch=trainX[rnd_index]
        y_batch=trainY[rnd_index]
        train_dict={x:x_batch,y:y_batch}
        session.run(optimizer,feed_dict=train_dict)
        c=session.run(loss,feed_dict=train_dict)/batch_size
        cost.append(c)
    return np.array(cost)

print_accuracy()

cost=train(1000)
plot_graph(cost)
print_accuracy()

print_confusion_matrix()